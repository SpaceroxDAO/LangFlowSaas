"""
Auto-generated component for: {{ agent_name }}
Created: {{ created_at }}
User ID: {{ user_id }}
Component ID: {{ component_id }}

This component is a custom AI agent created through Teach Charlie AI's
3-step Q&A wizard. It wraps Langflow's Agent functionality with prefilled
configuration from the user's answers.

DO NOT EDIT THIS FILE MANUALLY - it will be regenerated when the agent
is republished.

Component Standards: docs/04_LANGFLOW_COMPONENT_STANDARDS.md
"""
from langflow.custom import Component
from langflow.io import (
    MessageTextInput,
    Output,
    DropdownInput,
    SecretStrInput,
    SliderInput,
    IntInput,
    BoolInput,
    MultilineInput,
    HandleInput,
    DataInput,
)
from langflow.schema.message import Message
from langflow.field_typing import Tool
from langflow.field_typing.range_spec import RangeSpec


class {{ class_name }}(Component):
    """
    {{ description | replace('"', '\\"') }}

    This is a Teach Charlie AI agent component with prefilled configuration.
    """

    display_name = "{{ display_name | replace('"', '\\"') }}"
    description = "{{ description | replace('"', '\\"') }}"
    icon = "Bot"  # Lucide icon - see https://lucide.dev/icons
    name = "{{ class_name }}"
    version = "1.0.0"

    inputs = [
        # User Message Input - Required
        MessageTextInput(
            name="input_value",
            display_name="Message",
            info="Enter your message or question for {{ display_name | replace('"', '\\"') }}",
            required=True,
            tool_mode=True,
        ),

        # Model Configuration (visible by default)
        DropdownInput(
            name="model_provider",
            display_name="Model Provider",
            options=["OpenAI", "Anthropic", "Google", "Azure OpenAI"],
            value="{{ model_provider }}",
            info="Select the AI provider. Each provider requires its own API key.",
        ),
        DropdownInput(
            name="model_name",
            display_name="Model Name",
            options=[
                "gpt-4o-mini", "gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo",
                "claude-3-haiku-20240307", "claude-3-sonnet-20240229", "claude-3-opus-20240229",
                "gemini-1.5-flash", "gemini-1.5-pro",
            ],
            value="{{ model_name }}",
            info="Choose the specific model. Larger models are more capable but cost more.",
            combobox=True,
        ),
        SecretStrInput(
            name="api_key",
            display_name="API Key",
            info="Your API key for the selected provider. Leave blank to use environment variable (OPENAI_API_KEY, ANTHROPIC_API_KEY, or GOOGLE_API_KEY).",
            required=False,
        ),

        # Agent Instructions (PREFILLED from Q&A - the core value!)
        MultilineInput(
            name="agent_instructions",
            display_name="Agent Instructions",
            info="The system prompt that defines this agent's personality and behavior. This was generated from your Q&A answers.",
            value="""{{ system_prompt | replace('\\', '\\\\') | replace('"', '\\"') | replace('\n', '\\n') }}""",
            required=True,
        ),

        # Tools Input
        HandleInput(
            name="tools",
            display_name="Tools",
            input_types=["Tool", "BaseTool", "StructuredTool"],
            is_list=True,
            info="Connect tools here to give {{ display_name | replace('"', '\\"') }} additional capabilities like web search or calculations.",
        ),

        # Advanced Options (hidden by default)
        SliderInput(
            name="temperature",
            display_name="Temperature",
            value={{ temperature }},
            range_spec=RangeSpec(min=0, max=2, step=0.01),
            info="Controls randomness in responses. Lower = more deterministic, higher = more creative.",
            advanced=True,
        ),
        IntInput(
            name="max_tokens",
            display_name="Max Tokens",
            value={{ max_tokens }},
            range_spec=RangeSpec(min=1, max=128000),
            info="Maximum number of tokens in the response",
            advanced=True,
        ),
        IntInput(
            name="max_iterations",
            display_name="Max Iterations",
            value={{ max_iterations }},
            range_spec=RangeSpec(min=1, max=100),
            info="Maximum number of reasoning steps the agent can take",
            advanced=True,
        ),
        BoolInput(
            name="verbose",
            display_name="Verbose Mode",
            value={{ 'True' if verbose else 'False' }},
            info="Show detailed agent reasoning steps in the output",
            advanced=True,
        ),
        BoolInput(
            name="handle_parsing_errors",
            display_name="Handle Parsing Errors",
            value={{ 'True' if handle_parsing_errors else 'False' }},
            info="Gracefully handle LLM output parsing errors instead of failing",
            advanced=True,
        ),
        DataInput(
            name="chat_history",
            display_name="Chat History",
            is_list=True,
            info="Previous messages for multi-turn conversation context",
            advanced=True,
        ),
    ]

    outputs = [
        Output(
            display_name="Response",
            name="response",
            method="run_agent",
        ),
        Output(
            display_name="As Tool",
            name="component_as_tool",
            method="to_toolkit",
        ),
    ]

    def run_agent(self) -> Message:
        """Execute the agent with the configured settings."""
        import os

        self.log("Starting agent execution...")

        # Get API key from input or environment
        api_key = self.api_key
        if not api_key:
            if self.model_provider == "OpenAI":
                api_key = os.environ.get("OPENAI_API_KEY", "")
            elif self.model_provider == "Anthropic":
                api_key = os.environ.get("ANTHROPIC_API_KEY", "")
            elif self.model_provider == "Google":
                api_key = os.environ.get("GOOGLE_API_KEY", "")

        if not api_key:
            self.log("No API key provided", level="error")
            raise ValueError("No API key provided. Please enter your API key or set the environment variable.")

        self.log(f"Using provider: {self.model_provider}, model: {self.model_name}")

        # Select LLM based on provider
        try:
            if self.model_provider == "OpenAI":
                from langchain_openai import ChatOpenAI
                llm = ChatOpenAI(
                    model=self.model_name,
                    api_key=api_key,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens or None,
                )
            elif self.model_provider == "Anthropic":
                from langchain_anthropic import ChatAnthropic
                llm = ChatAnthropic(
                    model=self.model_name,
                    api_key=api_key,
                    temperature=self.temperature,
                    max_tokens=self.max_tokens or 4096,
                )
            elif self.model_provider == "Google":
                from langchain_google_genai import ChatGoogleGenerativeAI
                llm = ChatGoogleGenerativeAI(
                    model=self.model_name,
                    google_api_key=api_key,
                    temperature=self.temperature,
                    max_output_tokens=self.max_tokens or 4096,
                )
            else:
                # Default to OpenAI
                from langchain_openai import ChatOpenAI
                llm = ChatOpenAI(
                    model=self.model_name or "gpt-4o-mini",
                    api_key=api_key,
                    temperature=self.temperature,
                )
        except ImportError as e:
            self.log(f"Missing dependency for {self.model_provider}: {e}", level="error")
            raise ImportError(f"Missing dependency for {self.model_provider}. Please install the required package.")
        except Exception as e:
            self.log(f"Error initializing LLM: {e}", level="error")
            raise

        # Build tools list
        tools = self.tools if self.tools else []
        self.log(f"Tools configured: {len(tools)}")

        # Build chat history
        history = []
        if self.chat_history:
            for msg in self.chat_history:
                if hasattr(msg, 'data') and isinstance(msg.data, dict):
                    role = msg.data.get('role', 'user')
                    content = msg.data.get('content', '')
                    history.append((role, content))
            self.log(f"Chat history loaded: {len(history)} messages")

        try:
            if tools:
                # Use agent with tools
                self.log("Running agent with tools...")
                from langchain.agents import AgentExecutor, create_tool_calling_agent
                from langchain_core.prompts import ChatPromptTemplate

                prompt = ChatPromptTemplate.from_messages([
                    ("system", self.agent_instructions),
                    ("placeholder", "{chat_history}"),
                    ("human", "{input}"),
                    ("placeholder", "{agent_scratchpad}"),
                ])

                agent = create_tool_calling_agent(llm, tools, prompt)
                executor = AgentExecutor(
                    agent=agent,
                    tools=tools,
                    verbose=self.verbose,
                    max_iterations=self.max_iterations,
                    handle_parsing_errors=self.handle_parsing_errors,
                )

                result = executor.invoke({
                    "input": self.input_value,
                    "chat_history": history,
                })
                response_text = result.get("output", "")
            else:
                # No tools - just run LLM directly with system prompt
                self.log("Running LLM without tools...")
                from langchain_core.messages import SystemMessage, HumanMessage

                messages = [SystemMessage(content=self.agent_instructions)]
                for role, content in history:
                    if role == "user":
                        messages.append(HumanMessage(content=content))
                    else:
                        from langchain_core.messages import AIMessage
                        messages.append(AIMessage(content=content))
                messages.append(HumanMessage(content=self.input_value))

                response = llm.invoke(messages)
                response_text = response.content

            self.log("Agent execution completed successfully")
            return Message(text=response_text)

        except Exception as e:
            self.log(f"Error running agent: {e}", level="error")
            raise RuntimeError(f"Agent execution failed: {e}")

    def to_toolkit(self) -> list[Tool]:
        """Convert this component to a tool for use by other agents."""
        from langchain.tools import Tool as LangChainTool

        self.log("Converting component to tool...")

        def run_as_tool(query: str) -> str:
            """Run this agent as a tool."""
            original_input = self.input_value
            self.input_value = query
            try:
                result = self.run_agent()
                return result.text if hasattr(result, 'text') else str(result)
            finally:
                self.input_value = original_input

        self.log("Tool created successfully")
        return [LangChainTool(
            name="{{ class_name }}",
            description="{{ description | replace('"', '\\"') | truncate(200) }}",
            func=run_as_tool,
        )]
