{
  "id_prefix": "URLComponent",
  "component_type": "URLComponent",
  "display_name": "URL",
  "description": "Fetch content from one or more web pages, following links recursively.",
  "icon": "layout-template",
  "node": {
    "data": {
      "id": "URLComponent-Bj7cA",
      "node": {
        "base_classes": [
          "DataFrame",
          "Message"
        ],
        "beta": false,
        "category": "data",
        "conditional_paths": [],
        "custom_fields": {},
        "description": "Fetch content from one or more web pages, following links recursively.",
        "display_name": "URL",
        "documentation": "",
        "edited": false,
        "field_order": [
          "urls",
          "max_depth",
          "prevent_outside",
          "use_async",
          "format",
          "timeout",
          "headers",
          "filter_text_html",
          "continue_on_failure",
          "check_response_status",
          "autoset_encoding"
        ],
        "frozen": false,
        "icon": "layout-template",
        "key": "URLComponent",
        "last_updated": "2026-01-05T17:30:11.151Z",
        "legacy": false,
        "metadata": {
          "code_hash": "47d3ccb92d71",
          "dependencies": {
            "dependencies": [
              {
                "name": "requests",
                "version": "2.32.5"
              },
              {
                "name": "bs4",
                "version": "4.12.3"
              },
              {
                "name": "langchain_community",
                "version": "0.3.21"
              },
              {
                "name": "lfx",
                "version": "0.2.1"
              }
            ],
            "total_dependencies": 4
          },
          "module": "lfx.components.data_source.url.URLComponent"
        },
        "minimized": false,
        "output_types": [],
        "outputs": [
          {
            "allows_loop": false,
            "cache": true,
            "display_name": "Toolset",
            "group_outputs": false,
            "hidden": null,
            "loop_types": null,
            "method": "to_toolkit",
            "name": "component_as_tool",
            "options": null,
            "required_inputs": null,
            "selected": "Tool",
            "tool_mode": true,
            "types": [
              "Tool"
            ],
            "value": "__UNDEFINED__"
          }
        ],
        "pinned": false,
        "score": 2.220446049250313e-16,
        "template": {
          "_frontend_node_flow_id": {
            "value": "668898e3-0608-4c29-a77f-2b73f0047378"
          },
          "_frontend_node_folder_id": {
            "value": "afe00a3b-7693-4a52-b169-bfcd32c8ec22"
          },
          "_type": "Component",
          "autoset_encoding": {
            "_input_type": "BoolInput",
            "advanced": true,
            "display_name": "Autoset Encoding",
            "dynamic": false,
            "info": "If enabled, automatically sets the encoding of the request.",
            "list": false,
            "list_add_label": "Add More",
            "name": "autoset_encoding",
            "placeholder": "",
            "required": false,
            "show": true,
            "title_case": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "type": "bool",
            "value": true
          },
          "check_response_status": {
            "_input_type": "BoolInput",
            "advanced": true,
            "display_name": "Check Response Status",
            "dynamic": false,
            "info": "If enabled, checks the response status of the request.",
            "list": false,
            "list_add_label": "Add More",
            "name": "check_response_status",
            "placeholder": "",
            "required": false,
            "show": true,
            "title_case": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "type": "bool",
            "value": false
          },
          "code": {
            "advanced": true,
            "dynamic": true,
            "fileTypes": [],
            "file_path": "",
            "info": "",
            "list": false,
            "load_from_db": false,
            "multiline": true,
            "name": "code",
            "password": false,
            "placeholder": "",
            "required": true,
            "show": true,
            "title_case": false,
            "type": "code",
            "value": "import importlib\nimport re\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom langchain_community.document_loaders import RecursiveUrlLoader\n\nfrom lfx.custom.custom_component.component import Component\nfrom lfx.field_typing.range_spec import RangeSpec\nfrom lfx.helpers.data import safe_convert\nfrom lfx.io import BoolInput, DropdownInput, IntInput, MessageTextInput, Output, SliderInput, TableInput\nfrom lfx.log.logger import logger\nfrom lfx.schema.dataframe import DataFrame\nfrom lfx.schema.message import Message\nfrom lfx.utils.request_utils import get_user_agent\n\n# Constants\nDEFAULT_TIMEOUT = 30\nDEFAULT_MAX_DEPTH = 1\nDEFAULT_FORMAT = \"Text\"\n\n\nURL_REGEX = re.compile(\n    r\"^(https?:\\/\\/)?\" r\"(www\\.)?\" r\"([a-zA-Z0-9.-]+)\" r\"(\\.[a-zA-Z]{2,})?\" r\"(:\\d+)?\" r\"(\\/[^\\s]*)?$\",\n    re.IGNORECASE,\n)\n\nUSER_AGENT = None\n# Check if langflow is installed using importlib.util.find_spec(name))\nif importlib.util.find_spec(\"langflow\"):\n    langflow_installed = True\n    USER_AGENT = get_user_agent()\nelse:\n    langflow_installed = False\n    USER_AGENT = \"lfx\"\n\n\nclass URLComponent(Component):\n    \"\"\"A component that loads and parses content from web pages recursively.\n\n    This component allows fetching content from one or more URLs, with options to:\n    - Control crawl depth\n    - Prevent crawling outside the root domain\n    - Use async loading for better performance\n    - Extract either raw HTML or clean text\n    - Configure request headers and timeouts\n    \"\"\"\n\n    display_name = \"URL\"\n    description = \"Fetch content from one or more web pages, following links recursively.\"\n    documentation: str = \"https://docs.langflow.org/url\"\n    icon = \"layout-template\"\n    name = \"URLComponent\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            info=\"Enter one or more URLs to crawl recursively, by clicking the '+' button.\",\n            is_list=True,\n            tool_mode=True,\n            placeholder=\"Enter a URL...\",\n            list_add_label=\"Add URL\",\n            input_types=[],\n        ),\n        SliderInput(\n            name=\"max_depth\",\n            display_name=\"Depth\",\n            info=(\n                \"Controls how many 'clicks' away from the initial page the crawler will go:\\n\"\n                \"- depth 1: only the initial page\\n\"\n                \"- depth 2: initial page + all pages linked directly from it\\n\"\n                \"- depth 3: initial page + direct links + links found on those direct link pages\\n\"\n                \"Note: This is about link traversal, not URL path depth.\"\n            ),\n            value=DEFAULT_MAX_DEPTH,\n            range_spec=RangeSpec(min=1, max=5, step=1),\n            required=False,\n            min_label=\" \",\n            max_label=\" \",\n            min_label_icon=\"None\",\n            max_label_icon=\"None\",\n            # slider_input=True\n        ),\n        BoolInput(\n            name=\"prevent_outside\",\n            display_name=\"Prevent Outside\",\n            info=(\n                \"If enabled, only crawls URLs within the same domain as the root URL. \"\n                \"This helps prevent the crawler from going to external websites.\"\n            ),\n            value=True,\n            required=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"use_async\",\n            display_name=\"Use Async\",\n            info=(\n                \"If enabled, uses asynchronous loading which can be significantly faster \"\n                \"but might use more system resources.\"\n            ),\n            value=True,\n            required=False,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"format\",\n            display_name=\"Output Format\",\n            info=\"Output Format. Use 'Text' to extract the text from the HTML or 'HTML' for the raw HTML content.\",\n            options=[\"Text\", \"HTML\"],\n            value=DEFAULT_FORMAT,\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request in seconds.\",\n            value=DEFAULT_TIMEOUT,\n            required=False,\n            advanced=True,\n        ),\n        TableInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"The headers to send with the request\",\n            table_schema=[\n                {\n                    \"name\": \"key\",\n                    \"display_name\": \"Header\",\n                    \"type\": \"str\",\n                    \"description\": \"Header name\",\n                },\n                {\n                    \"name\": \"value\",\n                    \"display_name\": \"Value\",\n                    \"type\": \"str\",\n                    \"description\": \"Header value\",\n                },\n            ],\n            value=[{\"key\": \"User-Agent\", \"value\": USER_AGENT}],\n            advanced=True,\n            input_types=[\"DataFrame\"],\n        ),\n        BoolInput(\n            name=\"filter_text_html\",\n            display_name=\"Filter Text/HTML\",\n            info=\"If enabled, filters out text/css content type from the results.\",\n            value=True,\n            required=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"continue_on_failure\",\n            display_name=\"Continue on Failure\",\n            info=\"If enabled, continues crawling even if some requests fail.\",\n            value=True,\n            required=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"check_response_status\",\n            display_name=\"Check Response Status\",\n            info=\"If enabled, checks the response status of the request.\",\n            value=False,\n            required=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"autoset_encoding\",\n            display_name=\"Autoset Encoding\",\n            info=\"If enabled, automatically sets the encoding of the request.\",\n            value=True,\n            required=False,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Extracted Pages\", name=\"page_results\", method=\"fetch_content\"),\n        Output(display_name=\"Raw Content\", name=\"raw_results\", method=\"fetch_content_as_message\", tool_mode=False),\n    ]\n\n    @staticmethod\n    def validate_url(url: str) -> bool:\n        \"\"\"Validates if the given string matches URL pattern.\n\n        Args:\n            url: The URL string to validate\n\n        Returns:\n            bool: True if the URL is valid, False otherwise\n        \"\"\"\n        return bool(URL_REGEX.match(url))\n\n    def ensure_url(self, url: str) -> str:\n        \"\"\"Ensures the given string is a valid URL.\n\n        Args:\n            url: The URL string to validate and normalize\n\n        Returns:\n            str: The normalized URL\n\n        Raises:\n            ValueError: If the URL is invalid\n        \"\"\"\n        url = url.strip()\n        if not url.startswith((\"http://\", \"https://\")):\n            url = \"https://\" + url\n\n        if not self.validate_url(url):\n            msg = f\"Invalid URL: {url}\"\n            raise ValueError(msg)\n\n        return url\n\n    def _create_loader(self, url: str) -> RecursiveUrlLoader:\n        \"\"\"Creates a RecursiveUrlLoader instance with the configured settings.\n\n        Args:\n            url: The URL to load\n\n        Returns:\n            RecursiveUrlLoader: Configured loader instance\n        \"\"\"\n        headers_dict = {header[\"key\"]: header[\"value\"] for header in self.headers if header[\"value\"] is not None}\n        extractor = (lambda x: x) if self.format == \"HTML\" else (lambda x: BeautifulSoup(x, \"lxml\").get_text())\n\n        return RecursiveUrlLoader(\n            url=url,\n            max_depth=self.max_depth,\n            prevent_outside=self.prevent_outside,\n            use_async=self.use_async,\n            extractor=extractor,\n            timeout=self.timeout,\n            headers=headers_dict,\n            check_response_status=self.check_response_status,\n            continue_on_failure=self.continue_on_failure,\n            base_url=url,  # Add base_url to ensure consistent domain crawling\n            autoset_encoding=self.autoset_encoding,  # Enable automatic encoding detection\n            exclude_dirs=[],  # Allow customization of excluded directories\n            link_regex=None,  # Allow customization of link filtering\n        )\n\n    def fetch_url_contents(self) -> list[dict]:\n        \"\"\"Load documents from the configured URLs.\n\n        Returns:\n            List[Data]: List of Data objects containing the fetched content\n\n        Raises:\n            ValueError: If no valid URLs are provided or if there's an error loading documents\n        \"\"\"\n        try:\n            urls = list({self.ensure_url(url) for url in self.urls if url.strip()})\n            logger.debug(f\"URLs: {urls}\")\n            if not urls:\n                msg = \"No valid URLs provided.\"\n                raise ValueError(msg)\n\n            all_docs = []\n            for url in urls:\n                logger.debug(f\"Loading documents from {url}\")\n\n                try:\n                    loader = self._create_loader(url)\n                    docs = loader.load()\n\n                    if not docs:\n                        logger.warning(f\"No documents found for {url}\")\n                        continue\n\n                    logger.debug(f\"Found {len(docs)} documents from {url}\")\n                    all_docs.extend(docs)\n\n                except requests.exceptions.RequestException as e:\n                    logger.exception(f\"Error loading documents from {url}: {e}\")\n                    continue\n\n            if not all_docs:\n                msg = \"No documents were successfully loaded from any URL\"\n                raise ValueError(msg)\n\n            # data = [Data(text=doc.page_content, **doc.metadata) for doc in all_docs]\n            data = [\n                {\n                    \"text\": safe_convert(doc.page_content, clean_data=True),\n                    \"url\": doc.metadata.get(\"source\", \"\"),\n                    \"title\": doc.metadata.get(\"title\", \"\"),\n                    \"description\": doc.metadata.get(\"description\", \"\"),\n                    \"content_type\": doc.metadata.get(\"content_type\", \"\"),\n                    \"language\": doc.metadata.get(\"language\", \"\"),\n                }\n                for doc in all_docs\n            ]\n        except Exception as e:\n            error_msg = e.message if hasattr(e, \"message\") else e\n            msg = f\"Error loading documents: {error_msg!s}\"\n            logger.exception(msg)\n            raise ValueError(msg) from e\n        return data\n\n    def fetch_content(self) -> DataFrame:\n        \"\"\"Convert the documents to a DataFrame.\"\"\"\n        return DataFrame(data=self.fetch_url_contents())\n\n    def fetch_content_as_message(self) -> Message:\n        \"\"\"Convert the documents to a Message.\"\"\"\n        url_contents = self.fetch_url_contents()\n        return Message(text=\"\\n\\n\".join([x[\"text\"] for x in url_contents]), data={\"data\": url_contents})\n"
          },
          "continue_on_failure": {
            "_input_type": "BoolInput",
            "advanced": true,
            "display_name": "Continue on Failure",
            "dynamic": false,
            "info": "If enabled, continues crawling even if some requests fail.",
            "list": false,
            "list_add_label": "Add More",
            "name": "continue_on_failure",
            "placeholder": "",
            "required": false,
            "show": true,
            "title_case": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "type": "bool",
            "value": true
          },
          "filter_text_html": {
            "_input_type": "BoolInput",
            "advanced": true,
            "display_name": "Filter Text/HTML",
            "dynamic": false,
            "info": "If enabled, filters out text/css content type from the results.",
            "list": false,
            "list_add_label": "Add More",
            "name": "filter_text_html",
            "placeholder": "",
            "required": false,
            "show": true,
            "title_case": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "type": "bool",
            "value": true
          },
          "format": {
            "_input_type": "DropdownInput",
            "advanced": true,
            "combobox": false,
            "dialog_inputs": {},
            "display_name": "Output Format",
            "dynamic": false,
            "info": "Output Format. Use 'Text' to extract the text from the HTML or 'HTML' for the raw HTML content.",
            "name": "format",
            "options": [
              "Text",
              "HTML"
            ],
            "options_metadata": [],
            "placeholder": "",
            "required": false,
            "show": true,
            "title_case": false,
            "toggle": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "type": "str",
            "value": "Text"
          },
          "headers": {
            "_input_type": "TableInput",
            "advanced": true,
            "display_name": "Headers",
            "dynamic": false,
            "info": "The headers to send with the request",
            "input_types": [
              "DataFrame"
            ],
            "is_list": true,
            "list_add_label": "Add More",
            "name": "headers",
            "placeholder": "",
            "required": false,
            "show": true,
            "table_icon": "Table",
            "table_schema": {
              "columns": [
                {
                  "default": "None",
                  "description": "Header name",
                  "disable_edit": false,
                  "display_name": "Header",
                  "edit_mode": "popover",
                  "filterable": true,
                  "formatter": "text",
                  "hidden": false,
                  "name": "key",
                  "sortable": true,
                  "type": "str"
                },
                {
                  "default": "None",
                  "description": "Header value",
                  "disable_edit": false,
                  "display_name": "Value",
                  "edit_mode": "popover",
                  "filterable": true,
                  "formatter": "text",
                  "hidden": false,
                  "name": "value",
                  "sortable": true,
                  "type": "str"
                }
              ]
            },
            "title_case": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "trigger_icon": "Table",
            "trigger_text": "Open table",
            "type": "table",
            "value": [
              {
                "key": "User-Agent",
                "value": "langflow"
              }
            ]
          },
          "is_refresh": false,
          "max_depth": {
            "_input_type": "SliderInput",
            "advanced": false,
            "display_name": "Depth",
            "dynamic": false,
            "info": "Controls how many 'clicks' away from the initial page the crawler will go:\n- depth 1: only the initial page\n- depth 2: initial page + all pages linked directly from it\n- depth 3: initial page + direct links + links found on those direct link pages\nNote: This is about link traversal, not URL path depth.",
            "max_label": " ",
            "max_label_icon": "None",
            "min_label": " ",
            "min_label_icon": "None",
            "name": "max_depth",
            "placeholder": "",
            "range_spec": {
              "max": 5,
              "min": 1,
              "step": 1,
              "step_type": "float"
            },
            "required": false,
            "show": true,
            "slider_buttons": false,
            "slider_buttons_options": [],
            "slider_input": false,
            "title_case": false,
            "tool_mode": false,
            "type": "slider",
            "value": 1
          },
          "prevent_outside": {
            "_input_type": "BoolInput",
            "advanced": true,
            "display_name": "Prevent Outside",
            "dynamic": false,
            "info": "If enabled, only crawls URLs within the same domain as the root URL. This helps prevent the crawler from going to external websites.",
            "list": false,
            "list_add_label": "Add More",
            "name": "prevent_outside",
            "placeholder": "",
            "required": false,
            "show": true,
            "title_case": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "type": "bool",
            "value": true
          },
          "timeout": {
            "_input_type": "IntInput",
            "advanced": true,
            "display_name": "Timeout",
            "dynamic": false,
            "info": "Timeout for the request in seconds.",
            "list": false,
            "list_add_label": "Add More",
            "name": "timeout",
            "placeholder": "",
            "required": false,
            "show": true,
            "title_case": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "type": "int",
            "value": 30
          },
          "tools_metadata": {
            "_input_type": "ToolsInput",
            "advanced": false,
            "display_name": "Actions",
            "dynamic": false,
            "info": "Modify tool names and descriptions to help agents understand when to use each tool.",
            "is_list": true,
            "list_add_label": "Add More",
            "name": "tools_metadata",
            "override_skip": false,
            "placeholder": "",
            "real_time_refresh": true,
            "required": false,
            "show": true,
            "title_case": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "track_in_telemetry": false,
            "type": "tools",
            "value": [
              {
                "args": {
                  "urls": {
                    "default": "",
                    "description": "Enter one or more URLs to crawl recursively, by clicking the '+' button.",
                    "items": {
                      "type": "string"
                    },
                    "title": "Urls",
                    "type": "array"
                  }
                },
                "description": "Fetch content from one or more web pages, following links recursively.",
                "display_description": "Fetch content from one or more web pages, following links recursively.",
                "display_name": "fetch_content",
                "name": "fetch_content",
                "readonly": false,
                "status": true,
                "tags": [
                  "fetch_content"
                ]
              }
            ]
          },
          "urls": {
            "_input_type": "MessageTextInput",
            "advanced": false,
            "display_name": "URLs",
            "dynamic": false,
            "info": "Enter one or more URLs to crawl recursively, by clicking the '+' button.",
            "input_types": [],
            "list": true,
            "list_add_label": "Add URL",
            "load_from_db": false,
            "name": "urls",
            "placeholder": "Enter a URL...",
            "required": false,
            "show": true,
            "title_case": false,
            "tool_mode": true,
            "trace_as_input": true,
            "trace_as_metadata": true,
            "type": "str",
            "value": ""
          },
          "use_async": {
            "_input_type": "BoolInput",
            "advanced": true,
            "display_name": "Use Async",
            "dynamic": false,
            "info": "If enabled, uses asynchronous loading which can be significantly faster but might use more system resources.",
            "list": false,
            "list_add_label": "Add More",
            "name": "use_async",
            "placeholder": "",
            "required": false,
            "show": true,
            "title_case": false,
            "tool_mode": false,
            "trace_as_metadata": true,
            "type": "bool",
            "value": true
          }
        },
        "tool_mode": true
      },
      "showNode": true,
      "type": "URLComponent"
    },
    "dragging": false,
    "id": "URLComponent-Bj7cA",
    "measured": {
      "height": 290,
      "width": 320
    },
    "position": {
      "x": 1241.645826777893,
      "y": -34.70663041492506
    },
    "selected": false,
    "type": "genericNode"
  },
  "edge_output": {
    "name": "component_as_tool",
    "output_types": [
      "Tool"
    ]
  }
}